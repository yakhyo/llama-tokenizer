{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"new-llama-tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1, 6466, 0, 6592, 3952, 0, 1089, 1004, 1299, 0, 1027, 0, 6911, 0, 3806, 1058, 0, 2166, 16505], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "text = \"Men hammasini bilaman va men uchun kerakli\"\n",
    "tokens = tokenizer(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Men hammasini bilaman va men uchun kerakli ma'lumotlarni qayerdan topishni bilmayman\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(tokens['input_ids'])\n",
    "print(decoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [128000, 29819, 13824, 7044, 6729, 20934, 13005, 11412, 3026, 125738, 359, 34801, 587, 747, 7643, 64966, 372, 354, 75, 1923, 72, 2874, 1155, 36255, 1948, 819, 7907, 110269, 352, 1543], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Men hammasini bilaman va men uchun kerakli ma'lumotlarni qayerdan topishni bilmayman\n"
     ]
    }
   ],
   "source": [
    "# Decode token IDs back into text to see how it was processed\n",
    "decoded_text = tokenizer.decode(tokens['input_ids'])\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [128000, 40, 1120, 1390, 311, 1440, 323, 1457, 1070, 374, 264, 6908, 12330, 5108, 311, 603], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "text = \"I just want to know and now there is a huge wave coming to us\"\n",
    "tokens = tokenizer(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = AutoTokenizer.from_pretrained(\"new-llama-tokenizer\")\n",
    "wiki = AutoTokenizer.from_pretrained(\"new-llama-tokenizer_wiki\")\n",
    "instruct = AutoTokenizer.from_pretrained(\"behbudiy/Llama-3.1-8B-Instuct-Uz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['h', 'ow', 'Ġ', 'am', 'Ġ', 'I', 'Ġ', 'go', 'ing', 'Ġ', 'to', 'Ġ', 'be', 'Ġ', 'a', 'Ġ', 'book', 'w', 'or', 'm']\n"
     ]
    }
   ],
   "source": [
    "example = \"how am I going to be a bookworm\"\n",
    "\n",
    "tokens = news.tokenize(example)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bu', 'gun', 'Ġ', 'ni', 'malar', 'Ġ', 'qi', 'lishi', 'miz', 'Ġ', 'ker', 'ak']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = wiki.tokenize(example)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bug', 'un', 'Ġn', 'imal', 'ar', 'Ġq', 'il', 'ish', 'imiz', 'Ġker', 'ak']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = instruct.tokenize(example)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how', 'Ġam', 'ĠI', 'Ġgoing', 'Ġto', 'Ġbe', 'Ġa', 'Ġbook', 'worm']\n"
     ]
    }
   ],
   "source": [
    "example = \"how am I going to be a bookworm\"\n",
    "\n",
    "tokens = llama.tokenize(example)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how', 'Ġam', 'ĠI', 'Ġgoing', 'Ġto', 'Ġbe', 'Ġa', 'Ġbook', 'worm']\n"
     ]
    }
   ],
   "source": [
    "llama31 = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B\")\n",
    "example = \"how am I going to be a bookworm\"\n",
    "\n",
    "tokens = llama31.tokenize(example)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Men', 'da', '<unk>', 'ho', 'zir', '<unk>', 'ham', 'masi', '<unk>', 'juda', '<unk>', 'ham', '<unk>', 'ya', 'xshi', '<unk>', 'e', 'ka', 'nini', '<unk>', 'hamma', '<unk>', 'bil', 'maydi']\n"
     ]
    }
   ],
   "source": [
    "llama31 = AutoTokenizer.from_pretrained(\"uz_text_cls/\")\n",
    "example = \"Menda hozir hammasi juda ham yaxshi ekanini hamma bilmaydi\"\n",
    "\n",
    "tokens = llama31.tokenize(example)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M', 'enda', 'Ġ', 'h', 'ozir', 'Ġ', 'ham', 'masi', 'Ġ', 'j', 'uda', 'Ġ', 'ham', 'Ġ', 'y', 'axshi', 'Ġ', 'ek', 'an', 'ini', 'Ġ', 'ham', 'ma', 'Ġ', 'b', 'ilmaydi']\n"
     ]
    }
   ],
   "source": [
    "llama31 = AutoTokenizer.from_pretrained(\"uzbek_wiki_tokenizer\")\n",
    "example = \"Menda hozir hammasi juda ham yaxshi ekanini hamma bilmaydi\"\n",
    "\n",
    "tokens = llama31.tokenize(example)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
